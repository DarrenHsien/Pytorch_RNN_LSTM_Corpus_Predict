"""
相關圖片細節觀念 : http://colah.github.io/posts/2015-08-Understanding-LSTMs/

RNN的吸引力之一是這樣的想法，即它們可以將先前的信息連接到當前任務，
例如使用先前的視頻幀可能會有助於對當前幀的理解。如果RNN可以做到這一點，它們將非常有用。

有時，我們只需要查看最新信息即可執行當前任務。
例如，考慮一種語言模型，該模型試圖根據前一個單詞預測下一個單詞。
如果我們要預測“雲在天空中”的最後一個詞，很明顯，下一個詞將是“天空”。
在這種情況下，相關信息與所需信息之間的差距很小，RNN可以學習使用過去的信息。


但是在某些情況下，我們需要更多的上下文。
考慮嘗試預測文本“我在法國長大……我會說流利的法語”中的最後一個詞。最近的信息表明，
下一個詞可能是一種語言的名稱，但是如果我們想縮小哪種語言的範圍，我們需要從更遠的地方來追溯法國的情況。
相關信息和完全可能需要放大的點之間的差距。

不幸的是，隨著差距的擴大，RNN變得無法學習連接信息。

長短期記憶網絡（通常稱為“ LSTM”）是一種特殊的RNN，能夠學習長期依賴關係。
LSTM也具有這種鏈狀結構，但是重複模塊具有不同的結構。而不是只有一個神經網絡層，而是有四個以非常特殊的方式進行交互。

假設RNN網絡架構為
3(X input)-3(A hidden)-3(H output)
LSTM則是在A hidden內部節點下了不少功夫

A hidden內部節點透過縱向與橫向傳播增加了
-刪除訊息
-增加訊息
描述其中一個A節點內部運作其物理細節：
1.從單元中丟棄哪些信息
Ft = sigma( Wf ( H(t-1) + X(t) ) + bf )

2.要在單元中儲存的信息
C~t = tanh( Wc ( H(t-1) + X(t) ) + bc )
It = sigma( Wi ( H(t-1) + X(t) ) + bi )
整合前兩段獲得C(t) : 
就語言模型而言，這是我們實際刪除舊主題的性別信息並添加新信息的地方，正如我們在前面的步驟中確定的那樣
 刪除舊有訊息 + 添加新有訊息
C(t) = C(t-1) x Ft + It x C~t

3.決定要輸出的單元信息獲得H(t)
Ot = sigma( Wo ( H(t-1) + X(t) ) + bo )
H(t) = Ot x tanh(C(t))

以上為最原始的LSTM架構
在後續有須多相關LSTM變形體
-增添窺孔連接(peephole connections)
-耦合的忘記門和輸入門(coupled forget and input gates.)
-門控循環單元(Gated Recurrent Unit GRU)
"""